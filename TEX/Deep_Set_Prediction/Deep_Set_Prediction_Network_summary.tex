   %% LyX 2.3.4.2 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass[12pt,oneside,american,czech]{book}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[a4paper]{geometry}
\geometry{verbose,tmargin=4cm,bmargin=3cm,lmargin=3cm,rmargin=2cm,headheight=0.8cm,headsep=1cm,footskip=0.5cm}
\pagestyle{headings}
\setcounter{secnumdepth}{3}
\usepackage{url}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{setspace}
\usepackage{hyperref}
\usepackage{libertine}
\usepackage{comment}
\usepackage{calrsfs}
\usepackage{subfig}
\usepackage{cite}
\usepackage{url}
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}
\usepackage{algpseudocode}
\makeatletter
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Textclass specific LaTeX commands.
\newenvironment{lyxlist}[1]
	{\begin{list}{}
		{\settowidth{\labelwidth}{#1}
		 \setlength{\leftmargin}{\labelwidth}
		 \addtolength{\leftmargin}{\labelsep}
		 \renewcommand{\makelabel}[1]{##1\hfil}}}
	{\end{list}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% User specified LaTeX commands.
%% Font setup: please leave the LyX font settings all set to 'default'
%% if you want to use any of these packages:

%% Use Times New Roman font for text and Belleek font for math
%% Please make sure that the 'esint' package is turned off in the
%% 'Math options' page.
\usepackage[varg]{txfonts}

%% Use Utopia text with Fourier-GUTenberg math
%\usepackage{fourier}

%% Bitstream Charter text with Math Design math
%\usepackage[charter]{mathdesign}

%%---------------------------------------------------------------------

%% Make the multiline figure/table captions indent so that the second
%% line "hangs" right below the first one.
%\usepackage[format=hang]{caption}

%% Indent even the first paragraph in each section
\usepackage{indentfirst}

%%---------------------------------------------------------------------

%% Disable page numbers in the TOC. LOF, LOT (TOC automatically
%% adds \thispagestyle{chapter} if not overriden
%\addtocontents{toc}{\protect\thispagestyle{empty}}
%\addtocontents{lof}{\protect\thispagestyle{empty}}
%\addtocontents{lot}{\protect\thispagestyle{empty}}

%% Shifts the top line of the TOC (not the title) 1cm upwards 
%% so that the whole TOC fits on 1 page. Additional page size
%% adjustment is performed at the point where the TOC
%% is inserted.
%\addtocontents{toc}{\protect\vspace{-1cm}}

%%---------------------------------------------------------------------

% completely avoid orphans (first lines of a new paragraph on the bottom of a page)
\clubpenalty=9500

% completely avoid widows (last lines of paragraph on a new page)
\widowpenalty=9500

% disable hyphenation of acronyms
\hyphenation{CDFA HARDI HiPPIES IKEM InterTrack MEGIDDO MIMD MPFA DICOM ASCLEPIOS MedInria}

%%---------------------------------------------------------------------

%% Print out all vectors in bold type instead of printing an arrow above them
\renewcommand{\vec}[1]{\boldsymbol{#1}}
\newcommand{\R}{\mathbb{R}}
\newtheorem{theorem}{Theorem}
\newtheorem*{poz}{Poznámka}
\theoremstyle{definition}
\newtheorem{definition}{Definition}
\newtheorem{vet}{Věta}[section]
\newtheorem*{pří}{Příklad}
\newcommand{\N}{\mathbb{N}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\Z}{\mathbb{Z}}
%  Matematika
\newcommand{\ee}{\mathrm{e}} %eulerovo číslo
\newcommand{\ii}{\mathrm{i}} %imaginární jednotka
\newcommand{\inR}{\in \mathbb{R}}
\newcommand{\dd}{\mathrm{d}}
\newcommand\KLDeq{\mathrel{\stackrel{\makebox[0pt]{\mbox{\normalfont\tiny KLD}}}{\approx}}}
\newcommand*{\matr}[1]{\mathbfit{#1}}
\newcommand*{\tran}{^{\mkern-1.5mu\mathsf{T}}}
\newcommand*{\conj}[1]{\overline{#1}}
\newcommand*{\hermconj}{^{\mathsf{H}}}
\DeclareMathOperator{\tr}{tr}
\DeclareMathOperator*{\argmin}{arg\,min} 
\DeclareMathOperator*{\argmax}{arg\,max} 
\DeclareMathAlphabet{\pazocal}{OMS}{zplm}{m}{n}
% Jednotky
\newcommand{\unit}[1]{\,\mathrm{#1}} %jednotky zadávejte pomocí tohoto příkazu
\renewcommand{\deg}{\ensuremath{\mathring{\;}}} %symbol stupně
\newcommand{\celsius}{\ensuremath{\deg\mathrm{C}}} %stupně celsia

%(hodnota plus mínus chyba) jednotka
\newcommand{\hodn}[3]{(#1 \pm #2)\unit{#3}} 
\newcommand\norm[1]{\left\lVert#1\right\rVert}
%veličina [jednotka] do hlavičky tabulky
\newcommand{\tabh}[2]{\ensuremath{#1\,[\mathrm{#2}]}} 
% Replace standard \cite by the parenthetical variant \citep
%\renewcommand{\cite}{\citep}
\newcommand{\diag}{\mathop{\mathrm{diag}}}
\def\Var{{\textrm{Var}}\,}



\makeatother

\usepackage{babel}
\begin{document}
\section*{Deep Set Prediction Network}
\subsection*{Motivation}
Current approaches for predicting sets from feature vectors ignore the unordered
nature of sets and suffer from discontinuity issues as a result. The main difficulty in predicting sets comes from the ability to permute the elements in a set freely,
which means that there are $n!$ equally good solutions for a set of size n. Models that do not take this
set structure into account properly, such as MLPs \textit{(Multilayer perceptrons)}  or RNNs (\textit{recurrent neural networks}), result in discontinuities, which is the
reason why they struggle to solve simple toy set prediction tasks.
\subsection*{Background}
\textbf{Representation}\quad Sets of feature vectors with the feature vector describing
properties of the element. A set of size $n$
wherein each feature vector has dimensionality $d$ is represented as a matrix $Y$ $\in$ $\R^{d\times n}$ with the
elements as columns in an arbitrary order, $\textbf{Y} = \left[\textbf{y}_1,\dots \textbf{y}_n\right]$. To properly treat this as a set, it
is important to only apply operations with certain properties to it:\textbf{ permutation-invariance or
permutation-equivariance}. In other words, operations on sets should not rely on the arbitrary ordering
of the elements. \\
\begin{definition}[Permutation-invariant]
 A function $f : \R^{n\times c} \rightarrow \R^d$ is permutation-invariant iff it satisfies
\begin{equation}
f(\textbf{X}) = f(\textbf{PX}) 
\end{equation}
for all permutation matrices \textbf{P}.
\end{definition}

\begin{definition}[Permutation-equivariant]
 A function $g : \R^{n\times c} \rightarrow \R^{n\times d}$ is permutation-equivariant iff it satisfies
\begin{equation}
\textbf{P}g(\textbf{X}) = g(\textbf{PX}) 
\end{equation}
for all permutation matrices \textbf{P}.
\end{definition}


Set encoders (which turn such sets into feature vectors) are usually built by composing permutation-equivariant
operations with a permutation-invariant operation at the end. A simple example is the
model in
\begin{equation}
f\left(\textbf{Y}\right) = \sum_{i = 1}^n g\left(\textbf{y}_i\right)
\end{equation} where $g$ is a neural network. Because $g$ is applied to every element
individually, it does not rely on the arbitrary order of the elements. We can think of this as turning the
set $\left\lbrace\textbf{y}_i\right\rbrace_{i = 1}^{n} $ into $\left\lbrace g\left(\textbf{y}_i\right)\right\rbrace_{i = 1}^{n} $. This is permutation-equivariant because changing the order of elements
in the input set affects the output set in a predictable way. Next, the set is summed to produce a single
feature vector. Since summing is commutative, the output is the same regardless of what order the
elements are in. In other words, summing is permutation-invariant. This gives us an encoder that
produces the same feature vector regardless of the arbitrary order the set elements were stored in. \\

\textbf{Loss}\quad We need to compute a loss between predicted set $\hat{\textbf{Y}} = \left[\hat{\textbf{y}}_1,\dots,\hat{\textbf{y}}_n\right]$ and the target set 
$\textbf{Y} = \left[\textbf{y}_1,\dots \textbf{y}_n\right]$. The elements of each set are in an arbitrary order,
so we cannot simply compute a pointwise distance. We use a loss function that
is permutation-invariant in both its arguments. 
\begin{enumerate}
\item Chamfer loss 
\begin{equation}\label{Lcha}
 L_{\mathrm{cha}}\left(\textbf{Y},\hat{\textbf{Y}}\right) = \sum_i \min_j\norm{\hat{\textbf{y}}_i - \textbf{y}_j }^2 +   \sum_j \min_i\norm{\hat{\textbf{y}}_i - \textbf{y}_j }^2 
\end{equation}
Note that this does not work well for multi-sets, for example $\left[a,b,b\right]$ and $\left[a,a,b\right]$ - the loss is 0.
\item Hungarian loss 
 \begin{equation}\label{Lhun}
 L_{\mathrm{hun}}\left(\textbf{Y},\hat{\textbf{Y}}\right) =  \min_{\pi \in \Pi}\norm{\hat{\textbf{y}}_i - \textbf{y}_{\pi\left(i\right)} }^2   
\end{equation}
where $\Pi$ is the space of permutations. This has the benefit that every element in one set is associated to exactly one element in the
other set, which is not the case for the Chamfer loss.
\end{enumerate}
\section*{Deep Set Prediction Networks}
A model for decoding a feature vector into a set of
feature vectors. The main idea is based on the observation that the gradient of a set encoder with respect to the input
set is permutation-equivariant.
\begin{theorem}
The gradient of a permutation-invariant function $f: \R^{n\times c} \rightarrow \R^d$ with respect to its
input is permutation-equivariant
\begin{equation}
\textbf{P}\frac{\partial f\left(\textbf{X}\right)}{\partial \textbf{X}} = \textbf{P}\frac{\partial f\left(\textbf{P}\textbf{X}\right)}{\partial \textbf{P}\textbf{X}}.
\end{equation}
\end{theorem}
That means: \textit{to decode a feature vector into a set, we
can use gradient descent to find a set that encodes to that feature vector.}
This gives rise to a \textbf{nested optimisation} : an inner loop that changes a set to
encode more similarly to the input feature vector, and an outer loop that changes the weights of the
encoder to minimise a loss over a dataset.
\subsection*{Auto-encoding fixed size sets}
In a set auto-encoder, the goal is to turn the input set $\textbf{Y}$ into a small latent space $\textbf{z} = g_{enc}\left(\textbf{Y}\right)$ with
the encoder genc and turn it back into the predicted set $\hat{\textbf{Y}} = g_{dec}(\textbf{z})$ with the decoder $g_{dec}$. We define a representation loss and the corresponding decoder as
\begin{equation} 
 L_{\mathrm{repr}}\left(\textbf{z},\hat{\textbf{Y}}\right) =  \norm{g_{\mathrm{enc}}\left(\hat{\textbf{Y}}\right) - \textbf{z} }^2   
\end{equation}
\begin{equation}\label{gdec} 
g_{\mathrm{dec}}\left(\textbf{z}\right) = \argmin_{\hat{\textbf{Y}}}L_{\mathrm{repr}}\left(\textbf{z},\hat{\textbf{Y}}\right)
\end{equation}
In essence,  $L_{\mathrm{repr}}$ compares $\hat{\textbf{Y}}$ to $\textbf{Y}$ in the latent space.\\
Since the problem is non-convex when $g_{enc}$ is a neural network, it is infeasible to solve \eqref{gdec}
exactly. Instead, we perform gradient descent to approximate a solution. Starting from some initial
set $\hat{\textbf{Y}}^{\left(0\right)}$, gradient descent is performed for a fixed number of steps $T$ with the update rule
\begin{equation}
\hat{\textbf{Y}}^{(t+1)} =  \hat{\textbf{Y}}^{(t)} - \eta \cdot \frac{\partial L_{\mathrm{repr}}\left(\textbf{z},\hat{\textbf{Y}}^{(t)}\right)}{\partial \hat{\textbf{Y}}^{(t)}}
\end{equation}
with $\eta$ as the learning rate and the prediction being the final state, $g_{\mathrm{dec}}\left(\textbf{z}\right) = \hat{\textbf{Y}}^{(T)}$. This is the
aforementioned inner optimisation loop. \\
To obtain a good representation \textbf{z}, we still have to train the weights of $g_{enc}$. For this, we compute the auto-encoder objective  $L_{\mathrm{set}}\left(\textbf{Y},\hat{\textbf{Y}}^{(T)}\right)$ with $L_{\mathrm{set}} = L_{\mathrm{cha}}$ \eqref{Lcha} or $L_{\mathrm{hun}}$ \eqref{Lhun} and differentiate with respect
to the weights as usual, backpropagating through the steps of the inner optimisation. This is the
aforementioned outer optimisation loop.
\subsection*{Predicting sets from a feature vector}
Since the target representation \textbf{z} can come from a separate model (for example an image encoder $F$ encoding an image \textbf{x}), producing both the latent representation as well as decoding the set, is no longer possible in the general set prediction setup. When naïvely using $\textbf{z} = F(\textbf{x})$ as input to our decoder, our decoding process is unable to predict sets
correctly from it. Because the set encoder is no longer shared in our set decoder, there is no guarantee
that optimising $g_{enc}(\hat{\textbf{Y}})$ to match \textbf{z} converges towards \textbf{Y} (or a permutation thereof). To fix this, we
simply add a term to the loss of the outer optimisation that encourages $g_{enc}(\textbf{Y}) \approx \textbf{z}$ again. In other
words, the target set should have a very low representation loss itself. This gives us an additional
$L_{\mathrm{repr}}$ term in the loss function of the outer optimisation for supervised learning
\begin{equation}
\pazocal{L} = L_{\mathrm{set}}\left(\textbf{Y},\hat{\textbf{Y}}\right) + \lambda L_{\mathrm{repr}}\left(\textbf{Y},\textbf{z}\right).
\end{equation}
With this, minimising $L_{\mathrm{repr}}\left(\textbf{Y},\textbf{z}\right)$ in the inner optimisation 
will converge towards \textbf{Y}.\\

\begin{algorithm}[H]
\SetAlgoLined
\caption{One forward pass of the set prediction algorithm within the training loop.}
 $\textbf{z} = F(\textbf{x})$\\
$\hat{\textbf{Y}}^{\left(0\right)} \gets init$ \\
\For{t \gets 1,T}
        {
        $l \gets L_{\mathrm{repr}}\left(\textbf{Y}^{(t-1)},\textbf{z}\right)$\\
        $\hat{\textbf{Y}}^{(t)} \gets  \hat{\textbf{Y}}^{(t-1)} - \eta \cdot \frac{\partial l}{\partial \hat{\textbf{Y}}^{(t-1)}}$
        } 
predict $\hat{\textbf{Y}}^{\left(T\right)}$ \\
$\pazocal{L} = \frac{1}{T}\sum_{t = 0}^T L_{\mathrm{set}}\left(\textbf{Y},\hat{\textbf{Y}}^{(t)}\right) + \lambda L_{\mathrm{repr}}\left(\textbf{Y},\textbf{z}\right)$
\end{algorithm}




\end{document}